# Group Normalization in MXNet
[Group Normalization](https://arxiv.org/abs/1803.08494) (GN) was proposed by He Kaiming's team in March 2018. GN optimizes the disadvantage of BN in smaller mini-batch situations. The normalization of batch dimension will bring some problems - the inaccurate estimation of batch statistics will lead to the batch becoming smaller, and the error of BN will increase rapidly. In training large networks and transferring features to computer vision tasks (including detection, segmentation and video), memory consumption limits the use of only small batches of BN. A small Batch Size can cause Batch Normalization to fail.
Group Normalization (GN) is an alternative to BN. It first divides channels into groups, and then calculates the mean and method in each group for normalization. The calculation of GN is independent of Batch Size, and the accuracy is stable for different Batch Size. In addition, GN is easy to fine-tuning from the pre-trained model. The comparison between GN and BN is shown in the figure.
